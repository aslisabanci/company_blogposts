{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2xZX_4Ac9gTy"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aslisabanci/company_blogposts/blob/master/AIX_Demo_%7C_ONNX_%7C_Foundation_Models_for_Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.18.0 tf2onnx==1.9.3 onnxruntime==1.11.1 datarobot==2.28.0\n",
        "from transformers import AutoTokenizer, TFBertForQuestionAnswering\n",
        "import onnxruntime as ort\n",
        "import tf2onnx\n",
        "import numpy as np\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "ufS2RMjVj4Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FOUNDATION_MODEL = \"bert-large-uncased-whole-word-masking-finetuned-squad\""
      ],
      "metadata": {
        "id": "kMC2RHw_-tJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get BERT for question answering"
      ],
      "metadata": {
        "id": "LPuXPHHK8yzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(FOUNDATION_MODEL)\n",
        "model = TFBertForQuestionAnswering.from_pretrained(FOUNDATION_MODEL)\n",
        "BASE_PATH = \"/content/aix\"\n",
        "tokenizer.save_pretrained(BASE_PATH)\n",
        "model.save_pretrained(BASE_PATH)"
      ],
      "metadata": {
        "id": "snisPJdU9MAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Serialize Model"
      ],
      "metadata": {
        "id": "55J7RgBM86zA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m transformers.onnx --model=$BASE_PATH $BASE_PATH --feature=question-answering"
      ],
      "metadata": {
        "id": "Xvy4K0xhTStK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy to DataRobot"
      ],
      "metadata": {
        "id": "STo6t0PC9J7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assemble inference scripts & dependencies"
      ],
      "metadata": {
        "id": "2xZX_4Ac9gTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $BASE_PATH/custom.py\n",
        "\"\"\"\n",
        "Copyright 2021 DataRobot, Inc. and its affiliates.\n",
        "All rights reserved.\n",
        "This is proprietary source code of DataRobot, Inc. and its affiliates.\n",
        "Released under the terms of DataRobot Tool and Utility Agreement.\n",
        "\"\"\"\n",
        "import json\n",
        "import os.path\n",
        "import time\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "model_load_duration = 0\n",
        "\n",
        "\n",
        "def load_model(input_dir):\n",
        "    global model_load_duration\n",
        "    onnx_path = os.path.join(input_dir, \"model.onnx\")\n",
        "    tokenizer_path = os.path.join(input_dir)\n",
        "    start = time.time()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    sess = ort.InferenceSession(onnx_path)\n",
        "    model_load_duration = time.time() - start\n",
        "    log_for_drum(f\"load_model - Loading ONNX BERT took :{model_load_duration}\")\n",
        "    return sess, tokenizer\n",
        "\n",
        "\n",
        "def log_for_drum(msg):\n",
        "    os.write(1, f\"\\n{msg}\\n\".encode(\"UTF-8\"))\n",
        "\n",
        "\n",
        "def _get_answer_in_text(output, input_ids, idx, tokenizer):\n",
        "    answer_start = np.argmax(output[0], axis=1)[idx]\n",
        "    answer_end = (np.argmax(output[1], axis=1) + 1)[idx]\n",
        "    answer = tokenizer.convert_tokens_to_string(\n",
        "        tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "\n",
        "def score_unstructured(model, data, query, **kwargs):\n",
        "    global model_load_duration\n",
        "    sess, tokenizer = model\n",
        "\n",
        "    # Assume batch input is sent with mimetype:\"text/csv\"\n",
        "    # Treat as single prediction input if no mimetype is set\n",
        "    is_batch = kwargs[\"mimetype\"] == \"text/csv\"\n",
        "\n",
        "    if is_batch:\n",
        "        input_pd = pd.read_csv(io.StringIO(data), sep=\"|\")\n",
        "        input_pairs = list(zip(input_pd[\"context\"], input_pd[\"question\"]))\n",
        "\n",
        "        start = time.time()\n",
        "        inputs = tokenizer.batch_encode_plus(\n",
        "            input_pairs, add_special_tokens=True, padding=True, return_tensors=\"np\"\n",
        "        )\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        output = sess.run([\"start_logits\", \"end_logits\"], input_feed=dict(inputs))\n",
        "        responses = []\n",
        "        for i, row in input_pd.iterrows():\n",
        "            answer = _get_answer_in_text(output, input_ids[i], i, tokenizer)\n",
        "            response = {\n",
        "                \"context\": row[\"context\"],\n",
        "                \"question\": row[\"question\"],\n",
        "                \"answer\": answer,\n",
        "            }\n",
        "            responses.append(response)\n",
        "        pred_duration = time.time() - start\n",
        "        to_return = json.dumps(\n",
        "            {\n",
        "                \"predictions\": responses,\n",
        "                \"model_load_duration\": model_load_duration,\n",
        "                \"pred_duration\": pred_duration,\n",
        "            }\n",
        "        )\n",
        "    else:\n",
        "        data_dict = json.loads(data)\n",
        "        context, question = data_dict[\"context\"], data_dict[\"question\"]\n",
        "        start = time.time()\n",
        "        inputs = tokenizer(\n",
        "            question,\n",
        "            context,\n",
        "            add_special_tokens=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"np\",\n",
        "        )\n",
        "        input_ids = inputs[\"input_ids\"][0]\n",
        "        output = sess.run([\"start_logits\", \"end_logits\"], input_feed=dict(inputs))\n",
        "        answer = _get_answer_in_text(output, input_ids, 0, tokenizer)\n",
        "        pred_duration = time.time() - start\n",
        "        to_return = json.dumps(\n",
        "            {\n",
        "                \"context\": context,\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"model_load_duration\": model_load_duration,\n",
        "                \"pred_duration\": pred_duration,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    model_load_duration = 0  # reset this variable for subsequent calls\n",
        "    return to_return"
      ],
      "metadata": {
        "id": "CnNoNYawK_cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $BASE_PATH/requirements.txt\n",
        "transformers"
      ],
      "metadata": {
        "id": "TA18jMJ7L5Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload using DataRobot Python API"
      ],
      "metadata": {
        "id": "UZieTKaoB18p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deploy_to_datarobot(folder_path, env_name, model_name, descr):\n",
        "  API_TOKEN = \"YOUR_DATAROBOT_API_TOKEN\"\n",
        "  import datarobot as dr\n",
        "  dr.Client(token=API_TOKEN, endpoint='https://app.datarobot.com/api/v2/')\n",
        "  \n",
        "  onnx_execution_env = dr.ExecutionEnvironment.list(search_for=env_name)[0]\n",
        "  custom_model = dr.CustomInferenceModel.create(\n",
        "      name=model_name,\n",
        "      target_type=dr.TARGET_TYPE.UNSTRUCTURED,\n",
        "      description=descr,\n",
        "      language='python'\n",
        "  )\n",
        "  print(f\"Creating custom model version on {onnx_execution_env}...\")\n",
        "  \n",
        "  model_version = dr.CustomModelVersion.create_clean(\n",
        "      custom_model_id=custom_model.id,\n",
        "      base_environment_id=onnx_execution_env.id,\n",
        "      folder_path=folder_path,\n",
        "      maximum_memory=4096 * 1024 * 1024,\n",
        "  )\n",
        "  print(f\"Created {model_version}.\")\n",
        "\n",
        "  versions = dr.CustomModelVersion.list(custom_model.id)\n",
        "  sorted_versions = sorted(versions, key=lambda v: v.label)\n",
        "  latest_version = sorted_versions[-1]\n",
        "\n",
        "  print(\"Building the execution environment with dependency packages...\")\n",
        "  build_info = dr.CustomModelVersionDependencyBuild.start_build(\n",
        "      custom_model_id=custom_model.id,\n",
        "      custom_model_version_id=latest_version.id,\n",
        "      max_wait=3600,\n",
        "  )\n",
        "  print(f\"Environment build completed with {build_info.build_status}.\")\n",
        "\n",
        "  print(\"Creating model deployment...\")\n",
        "  default_prediction_server = dr.PredictionServer.list()[0]\n",
        "  deployment = dr.Deployment.create_from_custom_model_version(latest_version.id,\n",
        "                                                              label=model_name,\n",
        "                                                              description=descr,\n",
        "                                                              default_prediction_server_id=default_prediction_server.id,\n",
        "                                                              max_wait=600,\n",
        "                                                              importance=None)\n",
        "  \n",
        "  print(f\"{deployment} is ready!\")\n",
        "  return deployment"
      ],
      "metadata": {
        "id": "MY-Gro1AL9pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B29WgIP41yYi"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Usage:\n",
        "    python datarobot-predict.py <input-file> [mimetype] [charset]\n",
        "\n",
        "This example uses the requests library which you can install with:\n",
        "    pip install requests\n",
        "We highly recommend that you update SSL certificates with:\n",
        "    pip install -U urllib3[secure] certifi\n",
        "\"\"\"\n",
        "import sys\n",
        "import json\n",
        "import requests\n",
        "\n",
        "API_URL = 'https://mlops-dev.dynamic.orm.datarobot.com/predApi/v1.0/deployments/{deployment_id}/predictionsUnstructured'    # noqa\n",
        "API_KEY = 'YOUR_DATAROBOT_API_TOKEN'\n",
        "DATAROBOT_KEY = 'YOUR_DATAROBOT_KEY'\n",
        "\n",
        "# Don't change this. It is enforced server-side too.\n",
        "MAX_PREDICTION_FILE_SIZE_BYTES = 52428800  # 50 MB\n",
        "\n",
        "\n",
        "class DataRobotPredictionError(Exception):\n",
        "    \"\"\"Raised if there are issues getting predictions from DataRobot\"\"\"\n",
        "\n",
        "\n",
        "def make_datarobot_deployment_unstructured_predictions(data, deployment_id, mimetype, charset):\n",
        "    \"\"\"\n",
        "    Make unstructured predictions on data provided using DataRobot deployment_id provided.\n",
        "    See docs for details:\n",
        "         https://app.datarobot.com/docs/predictions/api/dr-predapi.html\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : bytes\n",
        "        Bytes data read from provided file.\n",
        "    deployment_id : str\n",
        "        The ID of the deployment to make predictions with.\n",
        "    mimetype : str\n",
        "        Mimetype describing data being sent.\n",
        "        If mimetype starts with 'text/' or equal to 'application/json',\n",
        "        data will be decoded with provided or default(UTF-8) charset\n",
        "        and passed into the 'score_unstructured' hook implemented in custom.py provided with the model.\n",
        "\n",
        "        In case of other mimetype values data is treated as binary and passed without decoding.\n",
        "    charset : str\n",
        "        Charset should match the contents of the file, if file is text.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data : bytes\n",
        "        Arbitrary data returned by unstructured model.\n",
        "\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    DataRobotPredictionError if there are issues getting predictions from DataRobot\n",
        "    \"\"\"\n",
        "    # Set HTTP headers. The charset should match the contents of the file.\n",
        "    headers = {\n",
        "        'Content-Type': '{};charset={}'.format(mimetype, charset),\n",
        "        'Authorization': 'Bearer {}'.format(API_KEY),\n",
        "        'DataRobot-Key': DATAROBOT_KEY,\n",
        "    }\n",
        "\n",
        "    url = API_URL.format(deployment_id=deployment_id)\n",
        "\n",
        "    # Make API request for predictions\n",
        "    predictions_response = requests.post(\n",
        "        url,\n",
        "        data=data,\n",
        "        headers=headers,\n",
        "    )\n",
        "    _raise_dataroboterror_for_status(predictions_response)\n",
        "    # Return raw response content\n",
        "    return predictions_response.content\n",
        "\n",
        "\n",
        "def _raise_dataroboterror_for_status(response):\n",
        "    \"\"\"Raise DataRobotPredictionError if the request fails along with the response returned\"\"\"\n",
        "    try:\n",
        "        response.raise_for_status()\n",
        "    except requests.exceptions.HTTPError:\n",
        "        err_msg = '{code} Error: {msg}'.format(\n",
        "            code=response.status_code, msg=response.text)\n",
        "        raise DataRobotPredictionError(err_msg)\n",
        "\n",
        "\n",
        "def datarobot_predict_file(filename, deployment_id, mimetype='text/csv', charset='utf-8'):\n",
        "    \"\"\"\n",
        "    Return an exit code on script completion or error. Codes > 0 are errors to the shell.\n",
        "    Also useful as a usage demonstration of\n",
        "    `make_datarobot_deployment_unstructured_predictions(data, deployment_id, mimetype, charset)`\n",
        "    \"\"\"\n",
        "    data = open(filename, 'rb').read()\n",
        "    data_size = sys.getsizeof(data)\n",
        "    if data_size >= MAX_PREDICTION_FILE_SIZE_BYTES:\n",
        "        print((\n",
        "                  'Input file is too large: {} bytes. '\n",
        "                  'Max allowed size is: {} bytes.'\n",
        "              ).format(data_size, MAX_PREDICTION_FILE_SIZE_BYTES))\n",
        "        return 1\n",
        "    try:\n",
        "        predictions = make_datarobot_deployment_unstructured_predictions(data, deployment_id, mimetype, charset)\n",
        "        return predictions\n",
        "    except DataRobotPredictionError as exc:\n",
        "        pprint(exc)\n",
        "        return None\n",
        "\n",
        "def datarobot_predict(input_dict, deployment_id, mimetype='application/json', charset='utf-8'):\n",
        "    \"\"\"\n",
        "    Return an exit code on script completion or error. Codes > 0 are errors to the shell.\n",
        "    Also useful as a usage demonstration of\n",
        "    `make_datarobot_deployment_unstructured_predictions(data, deployment_id, mimetype, charset)`\n",
        "    \"\"\"\n",
        "    data = json.dumps(input_dict).encode(charset)\n",
        "    data_size = sys.getsizeof(data)\n",
        "    if data_size >= MAX_PREDICTION_FILE_SIZE_BYTES:\n",
        "        print((\n",
        "                  'Input file is too large: {} bytes. '\n",
        "                  'Max allowed size is: {} bytes.'\n",
        "              ).format(data_size, MAX_PREDICTION_FILE_SIZE_BYTES))\n",
        "        return 1\n",
        "    try:\n",
        "        predictions = make_datarobot_deployment_unstructured_predictions(data, deployment_id, mimetype, charset)\n",
        "        return json.loads(predictions)['answer']\n",
        "    except DataRobotPredictionError as exc:\n",
        "        pprint(exc)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create deployment"
      ],
      "metadata": {
        "id": "9zkbMhpf-OiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deployment = deploy_to_datarobot(BASE_PATH, \n",
        "                                 \"ONNX\", \n",
        "                                 \"bert-onnx-questionAnswering\", \n",
        "                                 \"Pretrained BERT model, fine-tuned on SQUAD for question answering\")"
      ],
      "metadata": {
        "id": "T2nnOuM2CJn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use in production"
      ],
      "metadata": {
        "id": "4SE8fNM2ojeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = {\"context\": \"Healthcare tasks (e.g., patient care via disease treatment) and biomedical research (e.g., scientific discovery of new therapies) require expert knowledge that is limited and expensive. Foundation models present clear opportunities in these domains due to the abundance of data across many modalities (e.g., images, text, molecules) to train foundation models, as well as the value of improved sample efficiency in adaptation due to the cost of expert time and knowledge. Further, foundation models may allow for improved interface design (ยง2.5: interaction) for both healthcare providers and patients to interact with AI systems, and their generative capabilities suggest potential for open-ended research problems like drug discovery. Simultaneously, they come with clear risks (e.g., exacerbating historical biases in medical datasets and trials). To responsibly unlock this potential requires engaging deeply with the sociotechnical matters of data sources and privacy as well as model interpretability and explainability, alongside effective regulation of the use of foundation models for both healthcare and biomedicine.\", \"question\": \"Where can we use foundation models?\"}\n",
        "pprint(test_input)"
      ],
      "metadata": {
        "id": "wnWMqo4rgjFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datarobot_predict(test_input, deployment.id)"
      ],
      "metadata": {
        "id": "pg4L1pbzQoQX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}